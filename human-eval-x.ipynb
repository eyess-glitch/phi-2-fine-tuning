{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T15:34:20.465177Z","iopub.status.busy":"2024-02-19T15:34:20.464822Z","iopub.status.idle":"2024-02-19T15:35:54.799380Z","shell.execute_reply":"2024-02-19T15:35:54.798265Z","shell.execute_reply.started":"2024-02-19T15:34:20.465147Z"},"trusted":true},"outputs":[],"source":["!pip install accelerate bitsandbytes datasets peft transformers einops"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T15:36:27.242872Z","iopub.status.busy":"2024-02-19T15:36:27.242501Z","iopub.status.idle":"2024-02-19T15:36:52.167444Z","shell.execute_reply":"2024-02-19T15:36:52.166372Z","shell.execute_reply.started":"2024-02-19T15:36:27.242841Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# Load the Phi 2 model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"microsoft/phi-2\",\n","    trust_remote_code=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"/kaggle/input/train-lora-evol-seq-len-2048-r64\",\n","    device_map=\"auto\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T15:37:14.065491Z","iopub.status.busy":"2024-02-19T15:37:14.064760Z","iopub.status.idle":"2024-02-19T15:37:14.206630Z","shell.execute_reply":"2024-02-19T15:37:14.205851Z","shell.execute_reply.started":"2024-02-19T15:37:14.065456Z"},"trusted":true},"outputs":[],"source":["from transformers import GenerationConfig, TextStreamer, pipeline\n","\n","generation_config = GenerationConfig.from_pretrained(\"microsoft/phi-2\")\n","generation_config.max_new_tokens = 512\n","generation_config.temperature = 0.8\n","generation_config.do_sample = True\n","\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","\n","llm = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=False,\n","    generation_config=generation_config,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.eos_token_id,\n","    streamer=streamer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T16:02:23.235464Z","iopub.status.busy":"2024-02-19T16:02:23.235036Z","iopub.status.idle":"2024-02-19T16:02:23.256052Z","shell.execute_reply":"2024-02-19T16:02:23.255160Z","shell.execute_reply.started":"2024-02-19T16:02:23.235431Z"},"trusted":true},"outputs":[],"source":["# Apertura del nuovo file JSONL per la scrittura\n","output_file_path = \"samples.jsonl\"\n","output_file = open(output_file_path, 'w')\n","\n","base_path = \"/kaggle/input/human-eval-x-repo/codegeex/benchmark/humaneval-x\"\n","languages = [\"java\", \"js\", \"cpp\"]\n","\n","jsonl_files = [\n","    f\"{base_path}/{language}/data/humaneval_{language}.jsonl\"\n","    for language in languages\n","]\n","\n","for jsonl_file in jsonl_files:\n","    with open(jsonl_file, 'r') as input_file:\n","        for _ in range(83):\n","            line = input_file.readline()\n","            if not line:\n","                break  \n","            output_file.write(line)\n","\n","output_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T16:18:35.393703Z","iopub.status.busy":"2024-02-19T16:18:35.393368Z","iopub.status.idle":"2024-02-19T16:18:48.439161Z","shell.execute_reply":"2024-02-19T16:18:48.438335Z","shell.execute_reply.started":"2024-02-19T16:18:35.393677Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","file_path = \"samples.jsonl\"\n","output_file_path = \"generated_samples.jsonl\"\n","num_samples_per_task = 3\n","data = []\n","\n","with open(file_path, 'r') as file:\n","    for line in file:\n","        json_data = json.loads(line)\n","        for _ in range(num_samples_per_task):\n","            answer = llm(json_data['prompt'])[0]['generated_text']\n","            generated_sample = {\n","                \"task_id\": json_data['task_id'],\n","                \"generation\": answer\n","            }\n","            data.append(generated_sample)\n","        \n","with open(output_file_path, 'w') as output_file:\n","    for json_data in data:\n","        output_file.write(json.dumps(json_data) + '\\n')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4464519,"sourceId":7657222,"sourceType":"datasetVersion"},{"datasetId":4465493,"sourceId":7658738,"sourceType":"datasetVersion"},{"datasetId":4470567,"sourceId":7665754,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
