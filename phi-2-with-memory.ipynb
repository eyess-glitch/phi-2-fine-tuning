{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers datasets bitsandbytes accelerate peft einops\n","!pip install langchain openai weaviate-client tiktoken\n","!pip install -U angle-emb\n","!pip install chromadb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from transformers import GenerationConfig, TextStreamer, pipeline\n","from langchain.llms import HuggingFacePipeline \n","import torch\n","\n","# Load the Phi 2 model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"microsoft/phi-2\",\n","    trust_remote_code=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/phi-2\",\n","    device_map=\"auto\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","generation_config = GenerationConfig.from_pretrained(\"microsoft/phi-2\")\n","generation_config.max_new_tokens = 512\n","generation_config.temperature = 0.8\n","generation_config.do_sample = True\n","\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","\n","llm = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=False,\n","    generation_config=generation_config,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.eos_token_id,\n","    streamer=streamer,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T21:22:36.588413Z","iopub.status.busy":"2024-02-21T21:22:36.588046Z","iopub.status.idle":"2024-02-21T21:22:36.599235Z","shell.execute_reply":"2024-02-21T21:22:36.598380Z","shell.execute_reply.started":"2024-02-21T21:22:36.588384Z"},"trusted":true},"outputs":[],"source":["import chromadb\n","chroma_client = chromadb.Client()\n","collection = chroma_client.create_collection(name=\"prompts\", metadata={\"hnsw:space\": \"cosine\"})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from angle_emb import AnglE, Prompts\n","\n","encoder = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n","encoder.set_prompt(prompt=Prompts.C)\n","\n","id = 0\n","threshold = 0.40\n","\n","while True:\n","    query = input(\"\")\n","    query_embedding = encoder.encode({'text': query}, to_numpy=True)[0].tolist()\n","    \n","    result = collection.query(\n","        query_embeddings=[query_embedding],\n","        n_results=1,\n","    )\n","        \n","    print(result)\n","    \n","    context = \"\"\n","    \n","    if len(result['documents'][0]) > 0:\n","        similarity = result['distances'][0][0]\n","        if similarity < threshold:\n","            info = result['metadatas'][0][0]['question'] + result['documents'][0][0]\n","            context = f\"\"\"To answer this question, you can use the following informations but you must be coincise and correct : {info}\"\"\" \n","    \n","    if context != \"\":\n","        prompt = f\"\"\"Instruct: {query}{context}\\nOutput:\"\"\"\n","    else:\n","        prompt = f\"\"\"Instruct: {query}\\nOutput:\"\"\"\n","    \n","    answer = llm(prompt)[0]['generated_text']\n","    \n","    collection.add(\n","        embeddings=[query_embedding],\n","        documents=[answer],\n","        metadatas=[{\"question\": query}],\n","        ids=[f\"id{id}\"]\n","    )\n","    \n","    id += 1"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
